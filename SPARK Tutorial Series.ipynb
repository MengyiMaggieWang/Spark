{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([2,3,4])\n",
    "print (type(rdd))\n",
    "print (rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Operations in Spark Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simplerdd=sc.parallelize(x)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(simplerdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "print (simplerdd.collect()) #list ,not run collect on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "print (simplerdd.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4]\n"
     ]
    }
   ],
   "source": [
    "def square (number):\n",
    "    return number**2\n",
    "simpleRdd= sc.parallelize(x).map(square)  ##tranformation\n",
    "print(simpleRdd.take(2)) ##action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27, 64, 125, 216, 343]\n"
     ]
    }
   ],
   "source": [
    "simpleRdd = sc.parallelize(x).map(lambda x:x**3)\n",
    "print(simpleRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = sys.argv[1]    \n",
    "output = sys.argv[2]  ##2nd argument the system get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRIDE AND PREJUDICE',\n",
       " '',\n",
       " 'By Jane Austen',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Chapter 1',\n",
       " '',\n",
       " '',\n",
       " 'It is a truth universally acknowledged, that a single man in possession']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conf = SparkConf.setAppName('Word Count')\n",
    "                              \n",
    "rdd = sc.textFile('input.txt')\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRIDE', 'AND', 'PREJUDICE', '', 'By', 'Jane', 'Austen', '', '', '']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "counts.take(10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRIDE', 1),\n",
       " ('AND', 1),\n",
       " ('PREJUDICE', 1),\n",
       " ('', 1),\n",
       " ('By', 1),\n",
       " ('Jane', 1),\n",
       " ('Austen', 1),\n",
       " ('', 1),\n",
       " ('', 1),\n",
       " ('', 1)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1))\n",
    "counts.take(10)   ##tear off the line and then split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pride', 25),\n",
       " ('', 2465),\n",
       " ('jane', 152),\n",
       " ('chapter', 61),\n",
       " ('1', 1),\n",
       " ('is', 793),\n",
       " ('acknowledged,', 7),\n",
       " ('single', 8),\n",
       " ('in', 1834),\n",
       " ('possession', 8),\n",
       " ('of', 3561),\n",
       " ('good', 156),\n",
       " ('must', 299),\n",
       " ('however', 15),\n",
       " ('known', 47),\n",
       " ('views', 10),\n",
       " ('may', 177),\n",
       " ('his', 1240),\n",
       " ('entering', 9),\n",
       " ('neighbourhood,', 11)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: (word.lower(), 1)).reduceByKey(lambda x,y: x+y)\n",
    "counts.take(20)   ##CONVERT ALL TEXT INTO LOWERCASE => .LOWER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK Tutorial Series.ipynb  \u001b[34mstatename3\u001b[m\u001b[m/\r\n",
      "\u001b[34minput\u001b[m\u001b[m/                       \u001b[34mstatenames\u001b[m\u001b[m/\r\n",
      "\u001b[31minput.txt\u001b[m\u001b[m*                   \u001b[34mstatenames1\u001b[m\u001b[m/\r\n",
      "\u001b[34mstatename1\u001b[m\u001b[m/                  \u001b[34mstatenames2\u001b[m\u001b[m/\r\n",
      "\u001b[34mstatename2\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: input: File exists\r\n"
     ]
    }
   ],
   "source": [
    "mkdir input #make directory 'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv input.txt input  #move input.txt into 'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mengyiwang/Documents/workplace/pySpark/tutorial/input\n"
     ]
    }
   ],
   "source": [
    "cd input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mengyiwang/Documents/workplace/pySpark/tutorial\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK Tutorial Series.ipynb  \u001b[34mstatename3\u001b[m\u001b[m/\r\n",
      "\u001b[34minput\u001b[m\u001b[m/                       \u001b[34mstatenames\u001b[m\u001b[m/\r\n",
      "\u001b[34mstatename1\u001b[m\u001b[m/                  \u001b[34mstatenames1\u001b[m\u001b[m/\r\n",
      "\u001b[34mstatename2\u001b[m\u001b[m/                  \u001b[34mstatenames2\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatmap and coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=[1,2,3,4,5,6,7,8,9]\n",
    "rdd=sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [2, 6], [3, 12], [4, 20], [5, 30], [6, 42], [7, 56], [8, 72], [9, 90]]\n"
     ]
    }
   ],
   "source": [
    "series = rdd.map(lambda x:[x,(x+1)*x]) # map into tuple\n",
    "print (series.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 6, 3, 12, 4, 20, 5, 30, 6, 42, 7, 56, 8, 72, 9, 90]\n"
     ]
    }
   ],
   "source": [
    "series = rdd.flatMap(lambda x:[x,(x+1)*x])   #flatMap is case sensitive\n",
    "print (series.collect())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['New York', ['Syracuse', 'Rochester', 'Albany'], ['British Columbai', ['Vancouver', 'Survey', 'Burnaby']]]]\n"
     ]
    }
   ],
   "source": [
    "statecity =[['New York',['Syracuse','Rochester','Albany'],['British Columbai',['Vancouver','Survey','Burnaby']]]]\n",
    "rdd = sc.parallelize(statecity)\n",
    "print (rdd.take(2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['New York', 'Syracuse'], ['New York', 'Rochester'], ['New York', 'Albany']]\n"
     ]
    }
   ],
   "source": [
    "def flat(lst):\n",
    "    newlist = []\n",
    "    state = lst[0]\n",
    "    for ele in lst[1]:\n",
    "        newlist.append([state,ele])\n",
    "    return newlist\n",
    "print (flat(['New York',['Syracuse','Rochester','Albany']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['New York', 'Syracuse'], ['New York', 'Rochester'], ['New York', 'Albany']]]\n"
     ]
    }
   ],
   "source": [
    "print (rdd.map(flat).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['New York', 'Syracuse'], ['New York', 'Rochester'], ['New York', 'Albany']]\n"
     ]
    }
   ],
   "source": [
    "print (rdd.flatMap(flat).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "statename=['New York','California','Dakota','British Columbia']*25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(statename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "staterdd= sc.parallelize(statename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York',\n",
       " 'California',\n",
       " 'Dakota',\n",
       " 'British Columbia',\n",
       " 'New York',\n",
       " 'California',\n",
       " 'Dakota',\n",
       " 'British Columbia']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staterdd.take(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "staterdd.saveAsTextFile('statenames3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "staterdd.coalesce(1).saveAsTextFile('statename4')#decrease the number of partitions into 1 RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PySpark RDD Groupby function and reading documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weatherData = [\n",
    "               ['24/07/2018','Syracuse',20],\n",
    "               ['24/07/2018','Syracuse',21],\n",
    "               ['24/07/2018','Syracuse',22],\n",
    "               ['24/07/2018','Syracuse',23],\n",
    "               ['25/07/2018','Syracuse',23],\n",
    "                ['25/07/2018','Syracuse',23],\n",
    "               ['26/07/2018','Syracuse',24],\n",
    "                ['29/07/2018','Syracuse',29],\n",
    "               ['26/07/2018','Syracuse',23],\n",
    "                ['29/07/2018','Syracuse',23]\n",
    "              ]\n",
    "\n",
    "weatherRDD =sc.parallelize(weatherData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('24/07/2018',\n",
       "  [['24/07/2018', 'Syracuse', 20],\n",
       "   ['24/07/2018', 'Syracuse', 21],\n",
       "   ['24/07/2018', 'Syracuse', 22],\n",
       "   ['24/07/2018', 'Syracuse', 23]]),\n",
       " ('25/07/2018',\n",
       "  [['25/07/2018', 'Syracuse', 23], ['25/07/2018', 'Syracuse', 23]]),\n",
       " ('26/07/2018',\n",
       "  [['26/07/2018', 'Syracuse', 24], ['26/07/2018', 'Syracuse', 23]]),\n",
       " ('29/07/2018',\n",
       "  [['29/07/2018', 'Syracuse', 29], ['29/07/2018', 'Syracuse', 23]])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRDD.groupBy(lambda data:data[0]).mapValues(list).collect()  #case sensitive!! groupBy!!!\n",
    "##  key of the group by functions : data[0]-- group by date , data[1]--group by location , data[2]--group by number\n",
    "## data[0]--group by first column \n",
    "##grouo by as a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('24/07/2018', 'Syracuse'), 20),\n",
       " (('25/07/2018', 'Syracuse'), 23),\n",
       " (('26/07/2018', 'Syracuse'), 23),\n",
       " (('29/07/2018', 'Syracuse'), 23)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRDD.map(lambda data:((data[0],data[1]),data[2])).groupByKey().mapValues(min).collect() \n",
    "##select the min value of each group \n",
    "## (data[0],data[1]),data[2]) (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('24/07/2018', 'Syracuse'), 23),\n",
       " (('25/07/2018', 'Syracuse'), 23),\n",
       " (('26/07/2018', 'Syracuse'), 24),\n",
       " (('29/07/2018', 'Syracuse'), 29)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRDD.map(lambda data:((data[0],data[1]),data[2])).groupByKey().mapValues(max).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('24/07/2018', 'Syracuse'),\n",
       "  [['24/07/2018', 'Syracuse', 20],\n",
       "   ['24/07/2018', 'Syracuse', 21],\n",
       "   ['24/07/2018', 'Syracuse', 22],\n",
       "   ['24/07/2018', 'Syracuse', 23]]),\n",
       " (('25/07/2018', 'Syracuse'),\n",
       "  [['25/07/2018', 'Syracuse', 23], ['25/07/2018', 'Syracuse', 23]]),\n",
       " (('26/07/2018', 'Syracuse'),\n",
       "  [['26/07/2018', 'Syracuse', 24], ['26/07/2018', 'Syracuse', 23]]),\n",
       " (('29/07/2018', 'Syracuse'),\n",
       "  [['29/07/2018', 'Syracuse', 29], ['29/07/2018', 'Syracuse', 23]])]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRDD.groupBy(lambda data:(data[0],data[1])).mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reducebykey more efficient\n",
    "#map : get the format of key and value\n",
    "#lambda : the function to calculate the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('24/07/2018', 'Syracuse'), 20),\n",
       " (('25/07/2018', 'Syracuse'), 23),\n",
       " (('26/07/2018', 'Syracuse'), 23),\n",
       " (('29/07/2018', 'Syracuse'), 23)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRDD.map(lambda data:((data[0],data[1]),data[2])).reduceByKey(lambda a,b:min(a,b)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('24/07/2018', 'Syracuse'), 86),\n",
       " (('25/07/2018', 'Syracuse'), 46),\n",
       " (('26/07/2018', 'Syracuse'), 47),\n",
       " (('29/07/2018', 'Syracuse'), 52)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRDD.map(lambda data:((data[0],data[1]),data[2])).reduceByKey(lambda a,b:(a+b)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('24/07/2018', 'Syracuse'), (((20, 21), 22), 23)),\n",
       " (('25/07/2018', 'Syracuse'), (23, 23)),\n",
       " (('26/07/2018', 'Syracuse'), (24, 23)),\n",
       " (('29/07/2018', 'Syracuse'), (29, 23))]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherRDD.map(lambda data:((data[0],data[1]),data[2])).reduceByKey(lambda a,b:(a,b)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you are grouping in order to perform an aggregation (such as a sum or average)\n",
    "# over each key, using reduceByKey or aggregateByKey will provide much performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
